# Edge Impulse

Edge Impulse 是专为高通 Dragonwing 设备构建全新边缘AI模型的最简方案。它是一个端到端平台，可帮助您构建数据集、训练模型并以完整的硬件加速运行模型。它支持使用音频、图像和其他传感器数据构建 AI 模型，或者以多种格式引入您自己的模型。

## 训练 AI 模型

开始使用 Edge Impulse 进行构建：  
1️⃣ 确保已按设置步骤设置开发板。  
2️⃣ 在 [studio.edgeimpulse.com](https://studio.edgeimpulse.com) 注册一个免费的开发者账户。  
3️⃣ 在开发板的终端或 SSH 会话中，从 Nodesource PPA 安装 Node.js 22。

```bash
# Remove existing installation (if needed)
rm -f /usr/local/bin/node /usr/local/bin/npm

# Install Node.js v22
curl -fsSL https://deb.nodesource.com/setup_22.x | sudo -E bash -
sudo apt install -y nodejs

# Verify installation (might need to open a new terminal window)
node -v
# ... Should return v22.x.x
```

4️⃣ 接下来，安装 Edge Impulse for Linux，并将您的开发板连接到 Edge Impulse：

```bash
# Install the CLI
npm install -g edge-impulse-linux

# Connect to your project (to switch projects, add --clean)
edge-impulse-linux
```

![](https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-193d9c7cdfa12f3002b7c42a895d523cc7cf0353%2Fedgeimpulse1.png?alt=media "连接到 Edge Impulse 的 Qualcomm Dragonwing 开发板")

5️⃣ 按照 [端到端教程](https://docs.edgeimpulse.com/tutorials) 之一来构建您的第一个人工智能模型。

:::tip 
使用右上角的选择器选择您的 Qualcomm Dragonwing 开发板，并获取准确的性能信息。
:::

6️⃣ 在开发板的终端或 SSH 会话中运行您的模型：

```bash
edge-impulse-linux-runner
```

这样，您的模型便会自动完成构建并下载，随后在NPU上运行（仅适用于量化模型）。

或者，如果您想手动下载 EIM 文件，请在您 Edge Impulse 项目的**Deployment**页面中，搜索 "Linux (AARCH64 with Qualcomm QNN)" 这个选项。

![](https://3580193864-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxM5xrbdbelLSl7uN8oac%2Fuploads%2Fgit-blob-abc96bc0dca5f72946cf427ee415ef6271471fcc%2Fedgeimpulse2.png?alt=media "在 Qualcomm Dragonwing 开发板的 NPU 上运行的对象跟踪模型")

## 自带模型

Edge Impulse 也支持自带模型（BYOM），支持的格式包括 SavedModel、ONNX、TFLite、LiteRT 或 scikit-learn。通过 BYOM 部署的模型在 Dragonwing 平台上完全支持，并可利用 NPU 加速（适用于量化模型）。请参见 [Edge Impulse docs > Bring Your Own Model](https://docs.edgeimpulse.com/studio/projects/dashboard/byom)。

## 提示

### 查看 NPU 性能

Dragonwing 平台拥有强大的 NPU（神经处理单元），可以大幅加快 AI 推理速度。要了解 NPU 对性能的影响，您可以通过以下方式在 CPU 上运行模型：

```bash
edge-impulse-linux-runner --force-target runner-linux-aarch64
```

例如，在 RB3 Gen 2 Vision Kit 上使用一个具有 700 万参数的量化 YOLO 模型，CPU 每次推理需要 47 毫秒，而在NPU 上仅需 2 毫秒。
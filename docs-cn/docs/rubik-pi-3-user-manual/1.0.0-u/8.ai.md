---
# Display h2 to h4 headings
toc_min_heading_level: 2
toc_max_heading_level: 4
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# 人工智能

本章节将介绍Qualcomm AI Runtime SDK的使用流程，Qualcomm AI Runtime SDK可以帮助AI开发者便捷地使用高通的高性能机器学习推理硬件。它支持TensorFlow、PyTorch、ONNX和LiteRT等框架已经训练好的神经网络模型直接或是经过转换在RUBIK Pi 3上快速高效地运行。

## 概述

RUBIK Pi 3 Ubuntu  AI/ML 开发流程如下流程图所示：

![](images/diagram-24.jpg)

上面的AI/ML 开发流程大致分为两个步骤：

步骤 1

■ 编译并优化来自第三方 AI 框架的模型,以便在 RUBIK Pi 3上高效运行。例如,可以将 TensorFlow 模型导出为 TFLite 模型。可以对模型文件针对推理硬件来做量化、细调性能和精确度等特殊的定制操作。

步骤 2

编译应用程序,使用优化后的模型在设备上运行推理&#x20;

■ 将 AI 模型集成到用例 pipeline 中。

■ 交叉编译应用程序,生成使用依赖库的可执行二进制文件。

## 软件和硬件架构

### AI整体框架

![](images/diagram-19.jpg)

开发人员可以从ONNX、PyTorch、TensorFlow或TFLite中引入模型，使用Qualcomm AI Runtime SDK将这些模型高效地运行在RUBIK Pi 3的人工智能硬件-HTP（NPU）、GPU、CPU上。

### AI硬件

■ Qualcomm Kryo™ CPU- 一流的 CPU ,具有高性能和卓越的能效。

■ Qualcomm Adreno GPU- 适合在需要平衡功耗与性能的情况下执行 AI 工作负载。 AI 工作负载可以通过 OpenCL 内核进行加速。 GPU 还可用于加速模型预处理 / 后处理。

■ Qualcomm Hexagon 张量处理器 (HTP)- 又称 NPU/DSP/HMX ,适合低功耗、高性地能执行AI 工作负载。为优化性能,需要对预训练模型进行量化,使其达到支持的任一种精度。

### AI 软件

AI软件堆栈包含多种 SDK ,帮助AI开发者方便地利用RUBIK Pi 3的AI 硬件加速器的强大功能。开发人员可以自己选择的一种 SDK 来部署 AI 工作负载。预训练模型在运行之前需要将其转换为所选 SDK 所支持的可执行格式 (TFLite 模型除外)。 TFLite 模型可以使用 TFLite Delegate 直接运行。

■ **LiteRT**

LiteRT 模型可以使用以下 Delegate在 RUBIK Pi 3 的硬件上本地执行。

| Delegate                                 | 硬件加速器           |
| ---------------------------------------- | --------------- |
| AI Engine Direct Delegate (QNN Delegate) | CPU 、 GPU 和 HTP |
| XNNPACK Delegate                         | CPU             |
| GPU Delegate                             | GPU             |

■**Qualcomm 神经网络处理引擎**

Qualcomm 神经网络处理引擎 (Qualcomm Neural Processing Engine SDK ，也称为SNPE) 是一种用于执行深度神经网络的软件加速 运行时 。SNPE SDK提供相关工具来对神经网络进行转换、量化,并在 CPU 、 GPU 和 HTP 等硬件加速器上对其进行加速。

■ **Qualcomm AI Engine Direct (QNN)**

Qualcomm AI Engine Direct是为 AI/ML 场景用例使用Rubik Pi 3的AI加速器硬件而设计的一种软件架构。

该架构旨在提供统一的 API ,模块和可扩展的预加速库,从而基于这种可重用的结构打造全栈 AI 解决方案。它可为Qualcomm 神经网络处理 SDK(SNPE) 、 TFLite AI Engine Direct Delegate 等运行时提供支持。

■ **AI Model Efficiency Toolkit (AIMET)**

这是一个用于优化（压缩和量化）训练好的神经网络模型的开源库。并且该库是一个复杂的SDK旨在生成优化的量化模型，适用于高阶开发者。

## 编译和优化模型

![](images/diagram-1.jpg)

开发者可以使用以下两种方式中的任意一种方式来编译和优化自己的模型。

| **■ AI Hub**  |                                                                                |
| ------------- | ------------------------------------------------------------------------------ |
| **■ AI 软件堆栈** | 直接移植LiteRT AI的模型到RUBIK Pi 3设备上。使用一体化、易于定制的Qualcomm AI Runtime（QAIRT）SDK移植您的模型。 |

### AI Hub

AI Hub 提供了帮助开发人员针对视觉、音频和语音用例的机器学习模型在设备上进行优化、验证和部署的方法和途径。

![](images/diagram-2.jpg)

#### 环境配置

1. 在您的计算机上安装miniconda和配置Python环境。
     
     1. 安装miniconda。
        从[miniconda](https://www.anaconda.com/download)官网下载miniconda并安装。

     2. 打开命令行窗口。

<Tabs>
<TabItem value="win" label="Windows">

安装完成后,通过 Start 菜单打开 [Anaconda](https://docs.conda.io/projects/miniconda/en/latest/miniconda-install.html)提示符窗口。

</TabItem>
<TabItem value="maclinu" label="macOS/Linux">

安装完成后,打开一个新的 shell 窗口。

</TabItem>
</Tabs>

     3. 为AI Hub配置一个Python的虚拟环境

     ```bash
     conda activate
     conda create python=3.10 -n qai_hub
     conda activate qai_hub
     ```



2. 安装 AI Hub Python 客户端。

     ```bash
     pip3 install qai-hub
     pip3 install "qai-hub[torch]"
     ```

3. 登录 AI Hub。

     前往 [AI Hub](https://aihub.qualcomm.com/) 并使用 Qualcomm ID 登录,查看所创建作业的相关信息。

     登录后,导航至 Account > Settings > API Token 。此时应提供一个可用于配置客户端的 API 令牌。

   4. 在终端,使用以下命令通过 API 令牌配置客户端。

     ```bash
     qai-hub configure --api_token <INSERT_API_TOKEN>
     ```

     然后使用下面命令查看支持设备列表，验证 AI Hub Python 客户端是否安装成功：

     ```bash
     qai-hub list-devices
     ```

     出现如下结果说明 AI Hub Python 客户端安装成功：

     ![](images/diagram-20.jpg)


#### AI Hub 工作流程

##### 使用预优化模型

1. 导航到 [AI Hub Model Zoo](https://aihub.qualcomm.com/iot/models) ,访问适用于 RUBIK Pi 3的预优化模型。

2. 从左侧窗格中选择 Qualcomm QCS6490 作为芯片组,筛选适用于 RUBIK Pi 3的模型。

3. 从筛选结果视图中选择一个模型以导航到模型页面。

4. 在模型页面上,从下拉列表中选择 Qualcomm QCS6490 ,然后选择 TorchScript > TFLite 路径。

5. 点击下载按钮后开始模型下载。下载的模型已经过预先优化,可直接开发用户自己的应用程序。



##### 引入用户自己的模型


1. 选择 PyTorch 或 Onnx 格式的预训练模型。

2. 使用 Python API 将模型提交至 AI Hub 以进行编译或优化。提交编译作业时,必须选择设备或芯片组以及目标 runtime 才能编译模型。 RUBIK Pi 3支持TFLite runtime 。

     | **芯片组** | **Runtime** | **CPU**         | **GPU**   | **HTP**   |
     | ------- | ----------- | --------------- | --------- | --------- |
     | QCS6490 | TFLite      | INT8,FP16, FP32 | FP16,FP32 | NT8,INT16 |

     提交后, AI Hub 会为该作业生成一个唯一的 ID 。用户可以使用此作业 ID 查看作业详情。

3. AI Hub 会根据选择的设备和 runtime 对模型进行优化。或者,也可以提交作业在源自云设备集群且已经过配置的实际设备上对优化模型进行分析或推理(使用 Python API )。

     – 性能分析:在已配置的设备上对模型进行基准测试并提供统计数据,包括层级的平均推理时间、 runtime 配置等。

     – 推理:在推理作业执行过程中,使用优化模型基于提交的数据进行推理,即在已配置的云设备上运行该模型。

4. 提交的每项作业都可以在 AI Hub 门户中进行重新回顾。提交编译作业时,将会提供优化模型的可用下载链接。然后,该优化模型可以部署在 RUBIK Pi 3本地开发设备上。



### LiteRT



![](images/diagram-3.jpg)

LiteRT是一个用于在设备上进行AI推理的开源深度学习框架。 LiteRT 可优化模型的延迟、模型尺寸、功耗等,帮助开发人员在移动、嵌入式和边缘平台上运行自己的模型。RUBIK Pi 3支持通过下方列出的 TFLite Delegate 在本地执行 TFLite 模型。



| Delegate                                 | 加速器             |
| ---------------------------------------- | --------------- |
| AI Engine Direct Delegate (QNN Delegate) | CPU 、 GPU 和 HTP |
| XNNPack Delegate                         | CPU             |
| GPU Delegate                             | GPU             |



### Qualcomm AI Runtime SDK

Qualcomm AI Runtime SDK是一个多功能SDK，用于将ML模型移植到Qualcomm（RUBIK Pi 3）加速硬件上运行。SDK包含高通神经处理引擎（Qualcomm Neural Processing Engine ，即 SNPE）和AI Engine Direct（也称为QNN）所提供的工具，用于转换和量化PyTorch和TensorFlow等的模型，以及在CPU、GPU和HTP等运行时上运行这些模型。了解更多关于[SNPE](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/)和[QNN](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/)。

![](images/diagram-4.jpg)

## AI/ML 示例程序

AI/ML 示例程序展示了在 RUBIK Pi 3设备上从实时摄像头或是本地视频文件馈送数据然后运行模型的实际场景。下文将详述运行示例程序的步骤。

### 前提准备

##### 软件包的安装

参见[运行示例应用程序](3.run-sample-applications.md)中内容，确保里面的的示例程序可以正常运行。

桌面版本需要使用如下方式启动 Weston。

```bash
systemctl stop gdm
sudo dpkg-reconfigure weston-autostart
```

##### 模型文件和label文件下载

**LiteRT 的示例程序名称和对应的模型文件，label文件列表：**

| 示例应用程序                           | 所需模型                     | 所需label文件               |
|---------------------------------------|-----------------------------|----------------------------|
| image-classification-LiteRT-from-camera/file | resnet101-resnet101-w8a8.tflite | classification_0.labels    |
| object-detection-LiteRT-from-camera/file    | yolov8_det_quantized.tflite | yolov8.labels              |
| image-segmentation-LiteRT-from-camera/file  | deeplabv3_plus_mobilenet_quantized.tflite | deeplabv3_resnet50.labels |
| pose-detection-LiteRT-from-camera/file      | hrnet_pose_quantized.tflite | hrnet_pose.labels          |

**SNPE的示例程序名称和对应的模型文件，label文件列表：**

| 示例应用程序                           | 所需模型         | 所需label文件       |
|---------------------------------------|-----------------|--------------------|
| image-classification-LiteRT-from-camera/file | inceptionv3.dlc | classification.labels |
| object-detection-LiteRT-from-camera/file    | yolonas.dlc     | yolonas.labels      |

* [点击下载](https://thundercomm.s3.dualstack.ap-northeast-1.amazonaws.com/uploads/web/rubik-pi-3/tools/ai_sample_app_model_label.zip)示例程序需要的模型文件和label文件的压缩包ai\_sample\_app\_model\_label.zip。

* 您可以使用如下命令将压缩包内的所有模型文件和label文件一次性全部都推到板子的opt目录下

<Tabs>
<TabItem value="ubun" label="Ubuntu">

```bash
adb push ./* /opt
```
</TabItem>
<TabItem value="win" label="Windows">

```bash
adb push  /opt
```
</TabItem>
</Tabs>

* 您也可以在使用下面示例程序时单独推示例程序需要的模型文件和label文件到板子内的/opt目录下


### LiteRT示例程序:


#### 通过摄像头获取图像信息实现 AI 功能的示例程序

##### 图像分类(image-classification-LiteRT-from-camera)

该示例程序使用camera实时获取图像并传送给LiteRT使用HTP推理resnet101-resnet101-w8a8.tflite模型，然后将分类结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。


![](images/diagram-5.jpg)


* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push resnet101-resnet101-w8a8.tflite /opt
adb push classification.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0  ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;"  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants="Inception,q-offsets=<-38.0>,q-scales=<0.17039915919303894>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. 
```

* 效果图如下：

![](images/diagram-18.jpg)

##### 目标检测(object-detection-LiteRT-from-camera)

该示例程序使用camera实时获取图像并传送给LiteRT使用HTP推理yolov8\_det\_quantized.tflite模型，然后将目标检测结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-6.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push yolov8_det_quantized.tflite /opt
adb push yolov8.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants="YOLOv8,q-offsets=<21.0, 0.0, 0.0>,    q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-22.jpg)

##### 语义分割(image-segmentation-LiteRT-from-camera)

该示例程序使用camera实时获取图像并传送给LiteRT使用HTP推理deeplabv3\_plus\_mobilenet\_quantized.tflite模型，然后将语义分割结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-7.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push deeplabv3_plus_mobilenet_quantized.tflite /opt
adb push deeplabv3_resnet50.labels  /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants="deeplab,q-offsets=<-61.0>,q-scales=<0.06232302635908127>;" ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-23.jpg)

##### 姿态识别(pose-detection-LiteRT-from-camera)

该示例程序使用camera实时获取图像并传送给LiteRT使用HTP推理hrnet\_pose\_quantized.tflite模型，然后将人体姿态识别结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-8.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push hrnet_pose_quantized.tflite /opt
adb push hrnet_pose.labels  /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants="hrnet,q-offsets=<8.0>,q-scales=<0.0040499246679246426>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-21.jpg)

#### 通过录制好的 MP4 文件获取图像信息实现 AI 功能的示例程序

##### 图像分类(image-classification-LiteRT-from-file)

该示例程序使用 MP4 文件获取图像并传送给LiteRT使用HTP推理resnet101-resnet101-w8a8.tflite模型，然后将分类结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-9.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push resnet101-resnet101-w8a8.tflite /opt
adb push classification.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite name=tf_3 delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp,htp_device_id=(string)0,htp_performance_mode=(string)2,htp_precision=(string)1;"  model=/opt/resnet101-resnet101-w8a8.tflite ! queue ! qtimlvclassification threshold=51.0 results=5 module=mobilenet labels=/opt/classification_0.labels extra-operation=softmax constants="Inception,q-offsets=<-38.0>,q-scales=<0.17039915919303894>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer. 
```

* 效果图如下：

![](images/diagram-18.jpg)

##### 目标检测(object-detection-LiteRT-from-file)

该示例程序使用 MP4 文件获取图像并传送给LiteRT使用HTP推理yolov8\_det\_quantized.tflite模型，然后将物体检测结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-10.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push yolov8_det_quantized.tflite /opt
adb push yolov8.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/yolov8_det_quantized.tflite ! queue ! qtimlvdetection threshold=75.0 results=10 module=yolov8 labels=/opt/yolov8.labels constants="YOLOv8,q-offsets=<21.0, 0.0, 0.0>,    q-scales=<3.0546178817749023, 0.003793874057009816, 1.0>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-22.jpg)

##### 语义分割(image-segmentation-LiteRT-from-file)

该示例程序使用 MP4 文件获取图像并传送给LiteRT使用HTP推理deeplabv3\_plus\_mobilenet\_quantized.tflite模型，然后将语义分割结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。


![](images/diagram-11.jpg)


* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push deeplabv3_plus_mobilenet_quantized.tflite /opt
adb push deeplabv3_resnet50.labels  /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/deeplabv3_plus_mobilenet_quantized.tflite ! queue ! qtimlvsegmentation module=deeplab-argmax labels=/opt/deeplabv3_resnet50.labels constants="deeplab,q-offsets=<-61.0>,q-scales=<0.06232302635908127>;" ! video/x-raw,format=BGRA,width=256,height=144 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-23.jpg)

##### 姿态识别(pose-detection-LiteRT-from-file)

该示例程序使用 MP4 文件获取图像并传送给LiteRT使用HTP推理hrnet\_pose\_quantized.tflite模型，然后将人体姿态识别结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-12.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push hrnet_pose_quantized.tflite /opt
adb push hrnet_pose.labels  /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -v --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split ! queue ! qtivcomposer name=mixer sink_1::alpha=0.5 ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink="waylandsink fullscreen=true" split. ! queue ! qtimlvconverter ! queue ! qtimltflite delegate=external external-delegate-path=libQnnTFLiteDelegate.so external-delegate-options="QNNExternalDelegate,backend_type=htp;" model=/opt/hrnet_pose_quantized.tflite ! queue ! qtimlvpose threshold=51.0 results=2 module=hrnet labels=/opt/hrnet_pose.labels constants="hrnet,q-offsets=<8.0>,q-scales=<0.0040499246679246426>;" ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-21.jpg)


### SNPE示例程序

#### 通过摄像头获取图像信息实现 AI 功能的示例程序

##### 图像分类(image-classification-LiteRT-from-camera)

该示例程序使用camera实时获取图像并传送给SNPE使用HTP推理inceptionv3.dlc模型，然后将分类结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-13.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push inceptionv3.dlc /opt
adb push classification.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink="waylandsink sync=true fullscreen=true"  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-22.jpg)

##### 目标检测(object-detection-LiteRT-from-camera)

该示例程序使用camera实时获取图像并传送给SNPE使用HTP推理yolonas.labels模型，然后将目标检测结果和图像信息通过weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-14.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push yolonas.dlc /opt
adb push yolonas.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=1 qtiqmmfsrc name=camsrc camera=0 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink='waylandsink fullscreen=true sync=true' split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers="</heads/Mul, /heads/Sigmoid>" ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-21.jpg)

#### 通过录制好的 MP4 文件获取图像信息实现 AI 功能的示例程序

##### 图像分类(image-classification-LiteRT-from-file)

该示例程序使用 MP4 文件获取图像并传送给SNPE使用HTP推理inceptionv3.dlc模型，然后将分类结果和图像信息通过Weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-15.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push inceptionv3.dlc /opt
adb push classification.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! queue ! tee name=split ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true text-overlay=true video-sink="waylandsink sync=true fullscreen=true"  split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/inceptionv3.dlc ! queue ! qtimlvclassification threshold=40.0 results=2 module=mobilenet labels=/opt/classification.labels ! queue ! video/x-raw,format=BGRA,width=640,height=360 ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-18.jpg)

##### 目标检测(object-detection-LiteRT-from-file)

该示例程序使用 MP4 文件获取图像并传送给SNPE使用HTP推理yolonas.labels模型，然后将目标检测结果和图像信息通过Weston显示在显示器上。具体pipeline参见下面框图。

![](images/diagram-16.jpg)

* 从模型压缩包内将该示例需要的模型文件和label文件push到/opt目录下面

```bash
adb push yolonas.dlc /opt
adb push yolonas.labels /opt
```

* 执行下面命令运行该示例程序

```bash
sudo -i
export XDG_RUNTIME_DIR=/run/user/$(id -u ubuntu)/ && export WAYLAND_DISPLAY=wayland-1
gst-launch-1.0 -e --gst-debug=2 filesrc location=/opt/Draw_1080p_180s_30FPS.mp4 ! qtdemux ! queue ! h264parse ! v4l2h264dec capture-io-mode=4 output-io-mode=4 ! video/x-raw,format=NV12 ! tee name=split split. ! queue ! qtivcomposer name=mixer ! queue ! fpsdisplaysink sync=true signal-fps-measurements=true text-overlay=true video-sink='waylandsink fullscreen=true sync=true' split. ! queue ! qtimlvconverter ! queue ! qtimlsnpe delegate=dsp model=/opt/yolonas.dlc layers="</heads/Mul, /heads/Sigmoid>" ! queue ! qtimlvdetection module=yolo-nas labels=/opt/yolonas.labels ! video/x-raw,format=BGRA ! queue ! mixer.
```

* 效果图如下：

![](images/diagram-22.jpg)

### AI 相关插件的用途和功能

| 插件名字                 | 功能                                                                                                                                               |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| qtimlsnpe            | 负责snpe的dlc模型文件的加载和执行。它的输入接收来自预处理插件（qtimlvconverter）输出的张量，它的输出是传递给如qtimlvclassification/ qtimlvdetection/qtimlvsegmentation/  qtimlvpose插件的张量。    |
| qtimltflite          | 负责LiteRT的tflite模型文件的加载和执行。它的输入接收来自预处理插件（qtimlvconverter）输出的张量，它的输出是传递给如qtimlvclassification/ qtimlvdetection/qtimlvsegmentation/qtimlvpose插件的张量。 |
| qtimlvconverter      | 将传入视频缓冲区中的数据转换为神经网络张量,同时执行所需的格式转换和大小调整。                                                                                                          |
| qtimlvclassification | 对分类用例的输出张量进行后处理。                                                                                                                                 |
| qtimlvdetection      | 对检测用例的输出张量进行后处理。                                                                                                                                 |
| qtimlvsegmentation   | 对像素类用例的输出张量进行后处理,例如图像分割、深度图处理等。                                                                                                                  |
| qtimlvpose           | 对姿势估计用例的输出张量进行后处理。                                                                                                                               |
